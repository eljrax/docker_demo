---
# This is a quick and dirty playbook which sets up an openvswitch mesh 
# network between an arbitrary number of servers.
# It sets up Docker to use this network to allow cross-host
# communication for containers. 
# It sets up a docker-swarm cluster across the hosts, with the one having the
# host-var swarm_manager running the manager container.
# Check/modify the vars below to suit your needs. This is developed and tested
# on Ubuntu 14.04 VMs on the Rackspace cloud using a Cloud Network between the hosts.
#
# Also note that you need to do the subnetting yourself, and set the
# docker_range host-var in the inventory to split up the ranges docker will
# allocate to containers. 
#
# Note: It will open up the UFW firewall entirely on the interface you specify
# as 'cloud_network_iface'. This should be a private network shared between the
# hosts.
#
# Author: Erik Ljungstrom, Rackspace Ltd. 2015
#
- hosts: docker
  vars:
    # Directory where the openvswitch sources will be extracted to
    # The tarball will be downloaded to this directory
    ovs_dir: /usr/local/src
    ovs_pkg_name: openvswitch-2.4.0
    ovs_deb_names:
        - openvswitch-common_2.4.0-1_amd64.deb
        - openvswitch-switch_2.4.0-1_amd64.deb
    cloud_network_iface: eth2
    bridge_name: br0
    fluentd_log_directory: /var/log/fluentd_logs
    consul_data_directory: /var/lib/consul_data
    consul_recursors:
        - 83.138.151.81
        - 83.138.151.80
    swarm_node_list: /root/swarm_node_list
    swarm_cluster_id_file: /root/swarm_cluster_id
    docker_compose_version: 1.4.1
    docker_registry_host: el-docker-demo-1.example.com
    docker_registry_port: 443 
#   Uncomment  to require credentials to access the docker registry
#    docker_registry_users:
#        - name: docker
#          password: yourpasswordgoeshere
        
  remote_user: root
  tasks:

  # TODO: Get this into separate /etc/network/interfaces.d files
  - name: Generate OVS part of networking script
    template: src=templates/network_interfaces.j2 dest=/root/docker-interfaces

  - name: Check whether networking script has been merged in already
    shell: grep "OVS config - ansible" /etc/network/interfaces
    register: net_conf_check
    ignore_errors: true

  - name: Merge interface config with networks/interfaces if necessary
    shell: cat docker-interfaces >> /etc/network/interfaces
    when: net_conf_check|failed


    # We build a more recent openvswitch version than what's provided by Ubuntu
    # Tested with 2.4.0
  - name: Download openvswitch
    get_url: 
        # checksum=sha1:d091902579cf5101df851d2ec69c75a6bcbd49fc
        dest={{ ovs_dir}}/{{ ovs_pkg_name }}.tar.gz
        url=http://openvswitch.org/releases/{{ ovs_pkg_name }}.tar.gz

  - name: Extract ovs sources
    unarchive:
        creates={{ ovs_dir }}/{{ ovs_pkg_name }}
        dest={{ ovs_dir }}
        copy=no
        src={{ ovs_dir }}/{{ ovs_pkg_name}}.tar.gz

  - name: Install dependencies
    apt: pkg={{ item }} state=latest update_cache=yes cache_valid_time=600
    with_items:
        - build-essential
        - fakeroot
        - debhelper
        - autoconf
        - automake
        - bzip2
        - libssl-dev
        - openssl
        - graphviz
        - python-all
        - procps
        - python-qt4
        - python-zopeinterface
        - python-twisted-conch
        - libtool
        - python-pip

  - name: Check if debs already exists
    stat: path={{ ovs_dir}}/{{ item }}
    with_items: "{{ ovs_deb_names }}"
    register: debs_exists 

  # This will create ovs 2.4.0 .debs in {{ ovs_dir }} which will be installed on all servers if the .deb files does not exist already
  - name: Build .debs
    shell: chdir={{ ovs_dir }}/{{ ovs_pkg_name }} export DEB_BUILD_OPTIONS='parallel=8 nocheck' ; fakeroot debian/rules binary
    when: (debs_exists.results[0].stat.exists == False) or (debs_exists.results[1].stat.exists == False)

 
  - name: Add docker repository key
    apt_key: keyserver=p80.pool.sks-keyservers.net id=58118E89F3A912897C070ADBF76221572C52609D state=present 
  - name: Add docker repository
    apt_repository: repo="deb https://apt.dockerproject.org/repo ubuntu-trusty main" state=present
  
  - name: Install docker and bridge-utils (dependency for ovs)
    apt: pkg={{ item }} state=latest update_cache=yes cache_valid_time=600
    with_items:
        - bridge-utils
        - docker-engine
  

  # Install docker-compose
  - name: Install docker-compose
    pip: name=docker-compose

  # Install the OVS packages separately due to https://github.com/ansible/ansible/issues/9235
  - name: Install ovs common
    apt: deb={{ ovs_dir }}/{{ ovs_deb_names[0] }} state=present

  - name: Install ovs switch
    apt: deb={{ ovs_dir }}/{{ ovs_deb_names[1] }} state=present
 
    # Because http://askubuntu.com/questions/572497/cant-connect-to-pptp-vpn-with-ufw-enabled-on-ubuntu-14-04-with-kernel-3-18
  - name: Allow GRE traffic
    lineinfile: dest=/etc/ufw/before.rules insertbefore="# drop INVALID packets" line="-A ufw-before-input -p 47 -i {{ cloud_network_iface }} -j ACCEPT"

  - name: Open firewall on cloud-network interface
    ufw: direction=in 
        interface={{ cloud_network_iface }}
        rule=allow

  - name: Open firewall on cloud-network interface
    ufw: direction=out
        interface={{ cloud_network_iface }}
        rule=allow

  - name: Open firewall from containers to host
    ufw: direction=in
        interface=docker0
        rule=allow
 
  - name: Reload ufw
    ufw: state=reloaded


  - name: Install ruby on swarm manager node for fluentd
    apt: pkg={{ item }} state=latest update_cache=yes cache_valid_time=600
    with_items:
        - ruby
        - ruby-dev
    when: swarm_manager is defined

  - name: Install fluentd
    gem: name=fluentd state=present version=0.12.15 user_install=no 
    when: swarm_manager is defined

  - name: Create fluentd config file
    template: src=templates/fluent.conf dest=/etc/fluent.conf
    when: swarm_manager is defined 

  - name: Create fluentd user
    user: name=fluentd state=present createhome=yes home={{ fluentd_log_directory }}
        shell=/usr/sbin/nologin
    when: swarm_manager is defined

  - name: Create fluentd upstart config 
    template: src=templates/fluent-upstart.j2  dest=/etc/init/fluent.conf
    when: swarm_manager is defined

  - name: Enable fluent service
    service: name=fluent enabled=yes state=started
    when: swarm_manager is defined

   ################### DOCKER STUFF ####################
   
  - name: Write docker-defaults
    template: src=templates/docker-defaults.j2 dest=/etc/default/docker
    register: defaults_changed

  - name: Restart docker
    service: name=docker state=restarted
    when: defaults_changed.changed

    
    # This script is pure convenience and not part of day to day runnings
  - name: Put clean-up script on servers
    template: src=templates/cleandocker.sh.j2 dest=/usr/local/bin/cleandocker mode=770

    # We use file discovery, but using consul or etcd or something along those
    # lines is usually a bit more robust if you often add or remove hosts
  - name: Write swarm node list
    template: src=templates/swarm_nodes.j2 dest={{ swarm_node_list }}

  - name: Create consul config file
    template: src=templates/consul.json.j2 dest=/etc/consul.json

  - name: Create consul container start script (convenience only)
    template: src=templates/start_consul_container.sh.j2 dest=/usr/local/bin/start_consul_container.sh mode=770

  - name: Install docker-py as a workaround for Ansible issue
    pip: name=docker-py version=1.3.1

    # This puts a file in /root/swarm_manager which you can source
    # to set DOCKER_HOST in order to use swarm rather than the local
    # docker instance.
  - name: Put swarm_manager on hosts
    template: src=templates/swarm_manager.j2 dest=/root/swarm_manager

    # In this demo, we only use one consul server. That's not adviseable in
    # production. However an appropriate consul.json is put on the other hosts as
    # well, so to spin up additional servers, simply start a progrium/consul
    # container with -retry-join=$ip_of_container_on_swarm_manager
  - name: Start consul server container
    docker: image=progrium/consul name=consul_server state=started
        restart_policy=always
        command='-server -config-file=/etc/consul.json'
        volumes=/etc/consul.json:/etc/consul.json
    when: swarm_manager is defined
    
    # Set up docker swarm - set swarm_manager=1 on one of the hosts
    # in your inventory file
  - name: Start swarm manager container
    docker: image=swarm name=swarm_manager state=started
        restart_policy=always
        command='manage file:///cluster'
        volumes={{ swarm_node_list }}:/cluster
        ports=2376:2375
    when: swarm_manager is defined


####### Docker registry on swarm_manager #########

  - name: Set up docker registry on swarm_manager
    include: registry.yml
    when: swarm_manager is defined
